{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qidian.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1W1kyhv0wo8n",
        "nN1SDOQKeXBF",
        "uqx7Lm2-zF7Z",
        "7o02On6PdpGl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bojb2ZHJl-qx"
      },
      "source": [
        "# New Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Gtap8NIeYQ"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import bs4\n",
        "import pandas as pd\n",
        "import os\n",
        "import urllib.request\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TG1qVeb35J"
      },
      "source": [
        "## 2005-2007 (1-6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzR5OQr4eU8N"
      },
      "source": [
        "wd.get('https://web.archive.org/web/20071127152318/http://www.cmfu.com/topten.asp')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find_all(name='tr',attrs={'height':'25'}):\n",
        "  title.append(i.find_next('a').text)\n",
        "  author.append(i.find_next('a').find_next(name='td',attrs={'align':'center'}).text)\n",
        "  category.append(i.find_next('td').find_next('td').text)\n",
        "  book_id = i.find_next('a')['href'].partition('=')[-1]\n",
        "  image.append(f'https://web.archive.org/web/20071127152318im_/http://newauthor7.cmfu.com/books/{book_id}/{book_id}.jpg')\n",
        "  link.append(f'https://web.archive.org/web/20071127152318/http://www.cmfu.com/showbook.asp?Bl_id={book_id}')\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Id':book_id,'Title': title, 'Author':author,'Category':category,'Image':image,'Link':link}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "# save the dataframe\n",
        "df.to_excel('book_table_6.xlsx')\n",
        "\n",
        "# create the image folder\n",
        "os.mkdir('/content/images_6')\n",
        "\n",
        "# download images\n",
        "for i in range(len(link)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(image[i], f\"/content/images_6/book{i}.jpg\")\n",
        "  except:\n",
        "    try:\n",
        "      urllib.request.urlretrieve(image[i].replace('7.c','6.c'), f\"/content/images_6/book{i}.jpg\")\n",
        "    except:\n",
        "      try:\n",
        "        urllib.request.urlretrieve(image[i].replace('7.c','8.c'), f\"/content/images_6/book{i}.jpg\")\n",
        "      except:\n",
        "        try:\n",
        "          urllib.request.urlretrieve(image[i].replace('7.c','2.c'), f\"/content/images_6/book{i}.jpg\")\n",
        "        except:\n",
        "          try:\n",
        "            urllib.request.urlretrieve(image[i].replace('7.c','5.c'), f\"/content/images_6/book{i}.jpg\")\n",
        "          except:\n",
        "            try:\n",
        "              urllib.request.urlretrieve(image[i].replace('7.c','3.c'), f\"/content/images_6/book{i}.jpg\")\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/images_6.zip /content/images_6\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('images_6.zip')\n",
        "files.download('book_table_6.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDVD7Dy9b9gO"
      },
      "source": [
        "## 2008-2009 (7-10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4sxaXCac01u"
      },
      "source": [
        "wd.get('https://web.archive.org/web/20080421093642/http://www.qidian.com/Book/BookList.aspx')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find_all(name='a',attrs={'class':'tsn'}):\n",
        "  title.append(i.text)\n",
        "  category.append(i['title'])\n",
        "  try:\n",
        "    book_id = re.search('http://www.qidian.com/Book/(.*).aspx', i['href']).group(1)\n",
        "    image.append(f'https://web.archive.org/web/20080421093642im_/http://image.cmfu.com/books/{book_id}/{book_id}.jpg')\n",
        "    link.append(f'https://web.archive.org/web/20080421093642mp_/http://www.qidian.com/Book/{book_id}.aspx')\n",
        "  except:\n",
        "    image.append('NaN')\n",
        "    link.append('NaN')\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Id':book_id,'Title': title, 'Category':category,'Image':image,'Link':link}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "# save the dataframe\n",
        "df.to_excel('book_table_7.xlsx')\n",
        "\n",
        "# create the image folder\n",
        "os.mkdir('/content/images_7')\n",
        "\n",
        "# download images\n",
        "for i in range(len(link)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(image[i], f\"/content/images_7/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/images_7.zip /content/images_7\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('images_7.zip')\n",
        "files.download('book_table_7.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSGWav0KcKrQ"
      },
      "source": [
        "## 2010-2016"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ITLJFhTgzg"
      },
      "source": [
        "for i in soup.find_all(name='b',attrs={'class':'lei'}):\n",
        "  print(i.find_next('a').find_next('a')['href'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx6-9t5HTOPR"
      },
      "source": [
        "wd.get('https://web.archive.org/web/20170217233938/http://top.qdmm.com/MMWeb/TopDetail.aspx?TopType=7&Time=1')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find_all(name='b',attrs={'class':'lei'}):\n",
        "  category.append(i.text)\n",
        "  title.append(i.find_next('a').find_next('a')['title'])\n",
        "  author.append(i.find_next(name='b',attrs={'class':'author'}).text)\n",
        "  try:\n",
        "    book_id = re.search('http://www.qdmm.com/MMWeb/(.*).aspx', i.find_next('a').find_next('a')['href']).group(1)\n",
        "    image.append(f'https://web.archive.org/web/20170217233938im_/http://image.cmfu.com/books/{book_id}/{book_id}.jpg')\n",
        "    link.append(f'https://web.archive.org/web/20170217233938/http://www.qdmm.com/mmweb/{book_id}.aspx')\n",
        "  except:\n",
        "    image.append('NaN')\n",
        "    link.append('NaN')\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Title': title, 'Author':author,'Category':category,'Image':image,'Link':link}\n",
        "df_2017 = pd.DataFrame(d)\n",
        "df_2017"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNlyB0KVYt9H"
      },
      "source": [
        "# merge all dataframes\n",
        "df_final = pd.DataFrame()\n",
        "df_list = [df_2015, df_2016, df_2017]\n",
        "\n",
        "for i in df_list:\n",
        "  df_final=df_final.append(i)\n",
        "\n",
        "df_final = df_final.reset_index(drop=True)\n",
        "\n",
        "# save the dataframe\n",
        "df_final.to_excel('girl_monthly_2015_2020.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a2QMa5EY6bP"
      },
      "source": [
        "# create the image folder\n",
        "os.mkdir('/content/girl_monthly_2015_2020_images')\n",
        "\n",
        "# download images\n",
        "for i in range(len(df_final)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(df_final['Image'][i], f\"/content/girl_monthly_2015_2020_images/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/girl_monthly_2015_2020_images.zip /content/girl_monthly_2015_2020_images\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('girl_monthly_2015_2020_images.zip')\n",
        "files.download('girl_monthly_2015_2020.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62P-i-01Kytc"
      },
      "source": [
        "### girl site"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXwZut5yMMxD"
      },
      "source": [
        "# create individual dataframes\n",
        "wd.get('https://web.archive.org/web/20151022235243/http://www.qdmm.com/MMWeb/TopDetail.aspx?TopType=7&Time=1')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find_all(name='td',attrs={'width':'18%'}):\n",
        "  title.append(i.text)\n",
        "  author.append(i.find_next(name='td',attrs={'width':'15%'}).text)\n",
        "  category.append(i.previous_sibling.text)\n",
        "  book_id = book_id = re.search('http://www.qdmm.com/MMWeb/(.*).aspx', i.find_next('a')['href']).group(1)\n",
        "  image.append(f'https://web.archive.org/web/20140303110544im_/http://image.cmfu.com/books/{book_id}/{book_id}.jpg')\n",
        "  link.append(f'https://web.archive.org/web/20140303110544/http://www.qdmm.com/mmweb/{book_id}.aspx')\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Title': title, 'Author':author,'Category':category,'Image':image,'Link':link}\n",
        "df_2014a = pd.DataFrame(d)\n",
        "df_2014a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BONaxJSpOmpa"
      },
      "source": [
        "# merge all dataframes\n",
        "df_final = pd.DataFrame()\n",
        "df_list = [df_2011a, df_2011b, df_2012a, df_2012b, df_2013a, df_2013b,df_2014a]\n",
        "\n",
        "for i in df_list:\n",
        "  df_final=df_final.append(i)\n",
        "\n",
        "df_final = df_final.reset_index(drop=True)\n",
        "\n",
        "# save the dataframe\n",
        "df_final.to_excel('girl_monthly_2010_2014.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZZ9n8nCPBz6"
      },
      "source": [
        "# create the image folder\n",
        "os.mkdir('/content/girl_monthly_2010_2014_images')\n",
        "\n",
        "# download images\n",
        "for i in range(len(df_final)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(df_final['Image'][i], f\"/content/girl_monthly_2010_2014_images/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/girl_monthly_2010_2014_images.zip /content/girl_monthly_2010_2014_images\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('girl_monthly_2010_2014_images.zip')\n",
        "files.download('girl_monthly_2010_2014.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RhEGnOeIWNl"
      },
      "source": [
        "### main site"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ALmpXHk37Vb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "fffba1ff-015a-4e01-87b4-7bae348dfb2f"
      },
      "source": [
        "# create individual dataframes\n",
        "wd.get('https://web.archive.org/web/20110122124718/http://www.qdmm.com/MMWeb/MMBookSortList.aspx?Type=MV')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find(name='tbody').find_all(name='a',attrs={'class':'type'}):\n",
        "  category.append(i.text)\n",
        "\n",
        "for i in soup.find(name='tbody').find_all(name='a',attrs={'class':'name'}):\n",
        "  title.append(i.text)\n",
        "  book_id = re.search('http://www.qidian.com/Book/(.*).aspx', i['href']).group(1)\n",
        "  image.append(f'http://web.archive.org/web/20110122124718im_/http://image.cmfu.com/books/{book_id}/{book_id}.jpg')\n",
        "  link.append(f'https://web.archive.org/web/20110122124718/http://www.qidian.com/Book/{book_id}.aspx')\n",
        "\n",
        "for i in soup.find(name='tbody').find_all(name='a',attrs={'class':'author'}):\n",
        "  author.append(i.text)\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Title': title, 'Author':author,'Category':category,'Image':image,'Link':link}\n",
        "df_2011a = pd.DataFrame(d)\n",
        "df_2011a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Category</th>\n",
              "      <th>Image</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Title, Author, Category, Image, Link]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpFmGWYZABNV"
      },
      "source": [
        "# merge all dataframes\n",
        "df_final = pd.DataFrame()\n",
        "df_list = [df_2015, df_2016]\n",
        "\n",
        "for i in df_list:\n",
        "  df_final=df_final.append(i)\n",
        "\n",
        "df_final = df_final.reset_index(drop=True)\n",
        "\n",
        "# save the dataframe\n",
        "df_final.to_excel('monthly_pk_2015_2016.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6OVQqSwA8V8"
      },
      "source": [
        "# create the image folder\n",
        "os.mkdir('/content/monthly_pk_2015_2016_images')\n",
        "\n",
        "# download images\n",
        "for i in range(len(df_final)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(df_final['Image'][i], f\"/content/monthly_pk_2015_2016_images/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/monthly_pk_2015_2016_images.zip /content/monthly_pk_2015_2016_images\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('monthly_pk_2015_2016_images.zip')\n",
        "files.download('monthly_pk_2015_2016.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzR2Mf5esLcF"
      },
      "source": [
        "## 2017-2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zZRIXhXsR1n"
      },
      "source": [
        "wd.get('https://web.archive.org/web/20200829084158/https://www.qidian.com/rank/signnewbook')\n",
        "soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]\n",
        "\n",
        "for i in soup.find_all(name='div',attrs={'class':'book-mid-info'}):\n",
        "  title.append(i.find('h4').text)\n",
        "  author.append(i.find(name='p',attrs={'class':'author'}).text.partition('|')[0])\n",
        "  category.append(i.find(name='p',attrs={'class':'author'}).text.partition('|')[-1])\n",
        "  image.append(i.previous_sibling.previous_sibling.find('img')['src'])\n",
        "  link.append(i.previous_sibling.previous_sibling.find('a')['href'])\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Title': title, 'Author':author,'Category':category,'Image':image,'Link':link}\n",
        "df_2020b = pd.DataFrame(d)\n",
        "df_2020b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urp1adljwft_"
      },
      "source": [
        "# merge all dataframes\n",
        "df_final = pd.DataFrame()\n",
        "df_list = [df_2017,df_2018a,df_2018b,df_2019a,df_2019b,df_2020a,df_2020b]\n",
        "\n",
        "for i in df_list:\n",
        "  df_final=df_final.append(i)\n",
        "\n",
        "df_final = df_final.reset_index(drop=True)\n",
        "\n",
        "# save the dataframe\n",
        "df_final.to_excel('new_author_2017_2020.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiSoutUxTX-"
      },
      "source": [
        "# create the image folder\n",
        "os.mkdir('/content/new_author_2017_2020_images')\n",
        "\n",
        "# download images\n",
        "for i in range(len(df_final)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(df_final['Image'][i], f\"/content/new_author_2017_2020_images/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/new_author_2017_2020_images.zip /content/new_author_2017_2020_images\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('new_author_2017_2020_images.zip')\n",
        "files.download('new_author_2017_2020.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W1kyhv0wo8n"
      },
      "source": [
        "# Old Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN1SDOQKeXBF"
      },
      "source": [
        "## Javascript Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketXBw46qcuv"
      },
      "source": [
        "# CMFU 2005-2007 所有榜单\n",
        "cmfu= ['https://web.archive.org/web/20050301030212/http://cmfu.com/topten.asp',\n",
        "'https://web.archive.org/web/20051101011254/http://cmfu.com/topten.asp',\n",
        "'https://web.archive.org/web/20060430201710/http://cmfu.com/topten.asp',\n",
        "'https://web.archive.org/web/20061205090106/http://cmfu.com/topten.asp',\n",
        "'https://web.archive.org/web/20070406203456/http://cmfu.com/topten.asp',\n",
        "'https://web.archive.org/web/20071127152318/http://cmfu.com/topten.asp']\n",
        "\n",
        "# retrive javascript file to putlist with the web item id\n",
        "for i in range(len(cmfu)):\n",
        "  cmfu[i]='https://web.archive.org/web/'+cmfu[i][-41:-27]+'js_/http://cmfu.com/script/list_top3.js'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoUE9WFFlZCZ"
      },
      "source": [
        "# PARSE THE JAVASCRIPT IN THE CMFU LIST\n",
        "URL = cmfu[0]\n",
        "page = requests.get(URL)\n",
        "soup = page.text\n",
        "\n",
        "# use regex to retrieve substrings\n",
        "import re\n",
        "metadata_ls = re.findall('\\(.*?\\)',soup)\n",
        "del metadata_ls[0:12]\n",
        "del metadata_ls[-7:]\n",
        "\n",
        "# create a dataframe with all relevant metadata\n",
        "df = pd.DataFrame(metadata_ls)\n",
        "df = df[0].str.replace(\"'\",'')\n",
        "df = df.to_frame()\n",
        "df = df[0].str.split(',',expand=True)\n",
        "\n",
        "df_new = pd.DataFrame()\n",
        "df_new[['book_id','title','author','category']]  = df[[1,2,3,5]]\n",
        "\n",
        "df_new['image'] = 'https://web.archive.org/web/20050305213648im_/http://author.cmfu.com/bookimgs/' + df_new['book_id'] + '.jpg'\n",
        "df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcPlHqAAKs4I"
      },
      "source": [
        "# SAVE THE NEW DATAFRAME\n",
        "df_new.to_excel('book_table_2.xlsx')\n",
        "\n",
        "# create the image folder\n",
        "os.mkdir('/content/images_2')\n",
        "\n",
        "# download images\n",
        "for i in range(len(df_new)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(df_new['image'][i], f\"/content/images_2/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/images_2.zip /content/images_2\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('images_1.zip')\n",
        "files.download('book_table_2.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqx7Lm2-zF7Z"
      },
      "source": [
        "## Qidian 2008 - 2016 Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "PHwR3gFG_kNt",
        "outputId": "09cf5292-b950-420e-8361-f885a3c3e26f"
      },
      "source": [
        "# PARSE THE JAVASCRIPT IN THE CMFU LIST\n",
        "URL = 'https://web.archive.org/web/20080421093642js_/http://script.cmfu.com/script/BoardScript.js'\n",
        "page = requests.get(URL)\n",
        "soup = page.text\n",
        "\n",
        "# use regex to retrieve substrings\n",
        "import re\n",
        "metadata_ls = re.findall('\\(.*?\\)',soup)\n",
        "del metadata_ls[0:13]\n",
        "del metadata_ls[-6:]\n",
        "\n",
        "# create a dataframe with all relevant metadata\n",
        "df = pd.DataFrame(metadata_ls)\n",
        "df = df[0].str.replace(\"'\",'')\n",
        "df = df.to_frame()\n",
        "df = df[0].str.split(',',expand=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(1797814</td>\n",
              "      <td>召唤悍妞</td>\n",
              "      <td>2217003</td>\n",
              "      <td>共工天水</td>\n",
              "      <td>685213</td>\n",
              "      <td>31180029</td>\n",
              "      <td>卷二　八十八、自曝</td>\n",
              "      <td>8</td>\n",
              "      <td>东方玄幻)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(1775841</td>\n",
              "      <td>异世药王</td>\n",
              "      <td>2108391</td>\n",
              "      <td>独悠</td>\n",
              "      <td>655724</td>\n",
              "      <td>31180884</td>\n",
              "      <td>第五卷 灵御城！　第一八章 药师会堂</td>\n",
              "      <td>73</td>\n",
              "      <td>异界大陆)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(1209977</td>\n",
              "      <td>斗破苍穹</td>\n",
              "      <td>1019021</td>\n",
              "      <td>天蚕土豆</td>\n",
              "      <td>563468</td>\n",
              "      <td>29796715</td>\n",
              "      <td>作品相关　薰儿的图片</td>\n",
              "      <td>73</td>\n",
              "      <td>异界大陆)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(1264634</td>\n",
              "      <td>仙逆</td>\n",
              "      <td>1357513</td>\n",
              "      <td>耳根</td>\n",
              "      <td>416025</td>\n",
              "      <td>28536969</td>\n",
              "      <td>作品相关　三江访谈 感言</td>\n",
              "      <td>44</td>\n",
              "      <td>奇幻修真)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(1789892</td>\n",
              "      <td>最轮回</td>\n",
              "      <td>2228622</td>\n",
              "      <td>王袍</td>\n",
              "      <td>366735</td>\n",
              "      <td>31179707</td>\n",
              "      <td>正文　第112章 事实胜于雄辩</td>\n",
              "      <td>12</td>\n",
              "      <td>都市生活)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>(1515258</td>\n",
              "      <td>人生若只待初见</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>661</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>(1209977</td>\n",
              "      <td>斗破苍穹</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>635</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>(1601862</td>\n",
              "      <td>闪魂</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>631</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>(1454337</td>\n",
              "      <td>九转无天</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>553</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>(107580</td>\n",
              "      <td>凡人修仙传</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>528</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>)</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>907 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0        1        2     3  ...                   6   7      8     9\n",
              "0    (1797814     召唤悍妞  2217003  共工天水  ...           卷二　八十八、自曝   8  东方玄幻)  None\n",
              "1    (1775841     异世药王  2108391    独悠  ...  第五卷 灵御城！　第一八章 药师会堂  73  异界大陆)  None\n",
              "2    (1209977     斗破苍穹  1019021  天蚕土豆  ...          作品相关　薰儿的图片  73  异界大陆)  None\n",
              "3    (1264634       仙逆  1357513    耳根  ...        作品相关　三江访谈 感言  44  奇幻修真)  None\n",
              "4    (1789892      最轮回  2228622    王袍  ...     正文　第112章 事实胜于雄辩  12  都市生活)  None\n",
              "..        ...      ...      ...   ...  ...                 ...  ..    ...   ...\n",
              "902  (1515258  人生若只待初见        0        ...                       0      )  None\n",
              "903  (1209977     斗破苍穹        0        ...                       0      )  None\n",
              "904  (1601862       闪魂        0        ...                       0      )  None\n",
              "905  (1454337     九转无天        0        ...                       0      )  None\n",
              "906   (107580    凡人修仙传        0        ...                       0      )  None\n",
              "\n",
              "[907 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o02On6PdpGl"
      },
      "source": [
        "## Alternative Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Hd6uqrgYDk"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import bs4\n",
        "import pandas as pd\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "page = open('html_27.txt')\n",
        "soup = BeautifulSoup(page, \"html.parser\")\n",
        "soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qDJnnEyqJuk"
      },
      "source": [
        "soup.find(name='div', attrs={'class':'book-list'}).find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWz60w1vyQse"
      },
      "source": [
        "## Original Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD1IWEqw1iEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "824a0509-6e20-4919-b3a8-ae3d05e718c9"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import bs4\n",
        "import pandas as pd\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "# requeast the webpage where the content is located\n",
        "URL = \"http://web.archive.org/web/20040408204651/http://www.cmfu.com:80/ebook_list.asp\"\n",
        "page = requests.get(URL)\n",
        "soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "# identify the metadata to store\n",
        "link = [] ; image=[]; title = [] ; author = []; category=[]; date=[]\n",
        "\n",
        "# retrieve all links to be scraped\n",
        "for i in soup.find_all(name='tr', attrs={'bgcolor':'#ffffff'}):\n",
        "  link.append('http://web.archive.org/web/20040814050024/http://www.cmfu.com/' + i.find_next('a')['href'])  #link\n",
        "  ebook_id = i.find_next('a')['href'][-4:]\n",
        "  image.append(f'http://web.archive.org/web/20050222132957im_/http://www.cmfu.com/bookimage/{ebook_id}.jpg') #image\n",
        "  category.append(i.next.text) #category\n",
        "  title.append(i.find_next('a').text) #title\n",
        "  author.append(i.find_next('a').find_next(name='td',attrs={'align':'center'}).text) #author\n",
        "  date.append(i.find_next('a').find_next(name='td',attrs={'align':'center'}).find_next('td').text) #date\n",
        "\n",
        "# make a dataframe from all metadata\n",
        "book_id = list(range(0, len(link)))\n",
        "d = {'Id':book_id,'Title': title, 'Author':author,'Category':category,'Date':date,'Image':image,'Link':link}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "# save the dataframe\n",
        "df.to_excel('book_table.xlsx')\n",
        "\n",
        "# create the image folder\n",
        "os.mkdir('/content/images')\n",
        "\n",
        "# download images\n",
        "for i in range(len(link)):\n",
        "  try:\n",
        "    urllib.request.urlretrieve(image[i], f\"/content/images/book{i}.jpg\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# zip all files in the folder\n",
        "!zip -r /content/images.zip /content/images\n",
        "\n",
        "# download the zip file\n",
        "from google.colab import files\n",
        "files.download('images.zip')\n",
        "files.download('book_table.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/images/ (stored 0%)\n",
            "  adding: content/images/book18.jpg (deflated 2%)\n",
            "  adding: content/images/book3.jpg (deflated 2%)\n",
            "  adding: content/images/book4.jpg (deflated 76%)\n",
            "  adding: content/images/book9.jpg (deflated 2%)\n",
            "  adding: content/images/book1.jpg (deflated 2%)\n",
            "  adding: content/images/book29.jpg (deflated 2%)\n",
            "  adding: content/images/book15.jpg (deflated 76%)\n",
            "  adding: content/images/book19.jpg (deflated 76%)\n",
            "  adding: content/images/book16.jpg (deflated 75%)\n",
            "  adding: content/images/book22.jpg (deflated 76%)\n",
            "  adding: content/images/book21.jpg (deflated 2%)\n",
            "  adding: content/images/book0.jpg (deflated 2%)\n",
            "  adding: content/images/book14.jpg (deflated 76%)\n",
            "  adding: content/images/book2.jpg (deflated 2%)\n",
            "  adding: content/images/book28.jpg (deflated 76%)\n",
            "  adding: content/images/book7.jpg (deflated 2%)\n",
            "  adding: content/images/book17.jpg (deflated 79%)\n",
            "  adding: content/images/book8.jpg (deflated 2%)\n",
            "  adding: content/images/book12.jpg (deflated 76%)\n",
            "  adding: content/images/book24.jpg (deflated 45%)\n",
            "  adding: content/images/book13.jpg (deflated 76%)\n",
            "  adding: content/images/book11.jpg (deflated 1%)\n",
            "  adding: content/images/book26.jpg (deflated 76%)\n",
            "  adding: content/images/book23.jpg (deflated 76%)\n",
            "  adding: content/images/book10.jpg (deflated 2%)\n",
            "  adding: content/images/book5.jpg (deflated 76%)\n",
            "  adding: content/images/book20.jpg (deflated 76%)\n",
            "  adding: content/images/book6.jpg (deflated 2%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bb532ee7-9571-427a-815b-8074242d3c62\", \"images.zip\", 656977)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_13d88758-e3db-43e1-880a-3a5cdd15a910\", \"book_table.xlsx\", 7309)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}